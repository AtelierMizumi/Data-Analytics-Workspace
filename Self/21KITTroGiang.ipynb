{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60de6f9c-9832-4204-9c65-45d2ee0338d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hướng dẫn làm giữa kì và cuối kì theo như anh trợ giảng giải thích\n",
    "\n",
    "Sử dụng Google Colab: sử dụng Google Drive tạo ra file Python và Colab để tạo môi trường ảo trên Drive\n",
    "\n",
    "## Các yêu cầu cho các kì thi các kì\n",
    "\n",
    "### Giữa kì\n",
    "\n",
    "Một dữ liệu phải đặt 5 câu hỏi, sau khi đặt 5 câu hỏi thì sẽ giải quyết 5 câu hỏi \n",
    "\n",
    "Giả dụ: \n",
    "- Những cái trường (column) mình cần những cột nào, cột nào không cần thiết\n",
    "- Dữ liệu trong này ....\n",
    "- Những bệnh nhân này có trùng lặp hay không ?\n",
    "- Merge 2 dữ liệu (thực hành từ từ hôm nay)\n",
    "\n",
    "###  Cuối kì:\n",
    "- Thêm một phần là vẽ biểu đồ (phân tích dữ liệu không thể chỉ thể hiện giá trị, phải trực quan hóa nó lên\n",
    "- Đổi màu biểu đồ từ màu đỏ sang màu xanh chẳng hạn\n",
    "- Biểu diễn cái này sang biểu đồ hình cột \n",
    "- Có thể tra Google (thật ra cuối kì là phải học thuộc)\n",
    "\n",
    "Thầy có tool check bài trùng lặp xem có ai chôm ai ko\n",
    "Trình bày được đầy đủ bài, với các biểu đồ trực quan thì mặc định được 6 điểm\n",
    "Hỏi một câu một câu một điểm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "875b128f-033c-4c41-a29b-7c5926d3d57d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported file extension: ''. Supported file extensions are: .csv, .tsv, .json, .jsonl, .xml, .parquet, .feather, .sqlite, .sqlite3, .db, .db3, .s3db, .dl3, .xls, .xlsx, .xlsm, .xlsb, .odf, .ods, .odt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/thuanc177/Documents/Git/Data-Analytic/Self\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load the latest version\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m df \u001b[38;5;241m=\u001b[39m kagglehub\u001b[38;5;241m.\u001b[39mdataset_load(\n\u001b[1;32m     11\u001b[0m   KaggleDatasetAdapter\u001b[38;5;241m.\u001b[39mPANDAS,\n\u001b[1;32m     12\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmuqarrishzaib/tmdb-10000-movies-dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m   file_path,\n\u001b[1;32m     14\u001b[0m   \u001b[38;5;66;03m# Provide any additional arguments like \u001b[39;00m\n\u001b[1;32m     15\u001b[0m   \u001b[38;5;66;03m# sql_query or pandas_kwargs. See the \u001b[39;00m\n\u001b[1;32m     16\u001b[0m   \u001b[38;5;66;03m# documenation for more information:\u001b[39;00m\n\u001b[1;32m     17\u001b[0m   \u001b[38;5;66;03m# https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\u001b[39;00m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst 5 records:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter-da/lib/python3.12/site-packages/kagglehub/datasets.py:115\u001b[0m, in \u001b[0;36mdataset_load\u001b[0;34m(adapter, handle, path, pandas_kwargs, sql_query, hf_kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m adapter \u001b[38;5;129;01mis\u001b[39;00m KaggleDatasetAdapter\u001b[38;5;241m.\u001b[39mPANDAS:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkagglehub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datasets\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m kagglehub\u001b[38;5;241m.\u001b[39mpandas_datasets\u001b[38;5;241m.\u001b[39mload_pandas_dataset(\n\u001b[1;32m    116\u001b[0m         handle, path, pandas_kwargs\u001b[38;5;241m=\u001b[39mpandas_kwargs, sql_query\u001b[38;5;241m=\u001b[39msql_query\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     not_implemented_error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00madapter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not yet implemented\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter-da/lib/python3.12/site-packages/kagglehub/pandas_datasets.py:86\u001b[0m, in \u001b[0;36mload_pandas_dataset\u001b[0;34m(handle, path, pandas_kwargs, sql_query)\u001b[0m\n\u001b[1;32m     84\u001b[0m pandas_kwargs \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m pandas_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pandas_kwargs\n\u001b[1;32m     85\u001b[0m file_extension \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(path)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 86\u001b[0m read_function \u001b[38;5;241m=\u001b[39m _validate_read_function(file_extension, sql_query)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Now that everything has been validated, we can start downloading and processing\u001b[39;00m\n\u001b[1;32m     89\u001b[0m filepath \u001b[38;5;241m=\u001b[39m dataset_download(handle, path)\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter-da/lib/python3.12/site-packages/kagglehub/pandas_datasets.py:108\u001b[0m, in \u001b[0;36m_validate_read_function\u001b[0;34m(file_extension, sql_query)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_extension \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SUPPORTED_READ_FUNCTIONS_BY_EXTENSION:\n\u001b[1;32m    104\u001b[0m     extension_error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported file extension: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_extension\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupported file extensions are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(SUPPORTED_READ_FUNCTIONS_BY_EXTENSION\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m     )\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(extension_error_message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    110\u001b[0m read_function \u001b[38;5;241m=\u001b[39m SUPPORTED_READ_FUNCTIONS_BY_EXTENSION[file_extension]\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_function \u001b[38;5;129;01mis\u001b[39;00m wrapped_read_sql_query \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sql_query:\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported file extension: ''. Supported file extensions are: .csv, .tsv, .json, .jsonl, .xml, .parquet, .feather, .sqlite, .sqlite3, .db, .db3, .s3db, .dl3, .xls, .xlsx, .xlsm, .xlsb, .odf, .ods, .odt"
     ]
    }
   ],
   "source": [
    "# Install dependencies as needed:\n",
    "# pip install kagglehub[pandas-datasets]\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# Set the path to the file you'd like to load\n",
    "file_path = \"/home/thuanc177/Documents/Git/Data-Analytic/Self\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.dataset_load(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"muqarrishzaib/tmdb-10000-movies-dataset\",\n",
    "  file_path,\n",
    "  # Provide any additional arguments like \n",
    "  # sql_query or pandas_kwargs. See the \n",
    "  # documenation for more information:\n",
    "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
    ")\n",
    "\n",
    "print(\"First 5 records:\", df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
